{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import langdetect\n",
    "import glob\n",
    "nlp = spacy.load('en')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models.phrases import Phrases,Phraser\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"default\" scoring: <i>from “Efficient Estimaton of Word Representations in Vector Space” by\n",
    "Mikolov, et. al.: (count(worda followed by wordb) - min_count) * N / (count(worda) * count(wordb)) > threshold`, where N is the total vocabulary size.</i>\n",
    "\n",
    "\n",
    "\"npmi\" scoring: <i>normalized pointwise mutual information, from “Normalized (Pointwise) Mutual\n",
    "Information in Colocation Extraction” by Gerlof Bouma: ln(prop(worda followed by wordb) / (prop(worda)*prop(wordb))) / - ln(prop(worda followed by wordb) where prop(n) is the count of n / the count of everything in the entire corpus.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_gram_transformers(stream,n_gram = 3,scoring=\"default\",min_count=5,threshold=10,common_terms=None):\n",
    "    streams = [stream]    \n",
    "    grams = [stream]\n",
    "    for n in range(1,n_gram):\n",
    "        gram = Phraser(Phrases(streams[-1],scoring=scoring,min_count=min_count,threshold=threshold,common_terms=common_terms))\n",
    "        streams.append(list(gram[streams[-1]]))\n",
    "        grams.append(gram)\n",
    "        \n",
    "    return grams\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,to_bigrams,to_trigrams,to_quadgrams = generate_n_gram_transformers(sents_stream,n_gram=4,\n",
    "                                                   scoring=\"default\",min_count=30,\n",
    "                                                   threshold=10,common_terms=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_stream = list(to_quadgrams[to_trigrams[to_bigrams[sents_stream]]])\n",
    "tri_stream = list(to_trigrams[to_bigrams[sents_stream]])\n",
    "quad_sents = [' '.join(sent) for sent in quad_stream]\n",
    "tri_sents = [' '.join(sent) for sent in tri_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call_centre',\n",
       " 'accessibility',\n",
       " 'is',\n",
       " 'the',\n",
       " 'primary',\n",
       " 'frustration',\n",
       " 'for',\n",
       " 'canadians',\n",
       " 'and',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'be',\n",
       " 'improved']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_stream[random.randint(0,len(quad_stream))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Words Using Part of Speech Tagging and Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_childrens(childrens):\n",
    "    \n",
    "    important_childrens = []\n",
    "    \n",
    "    child_tokens = [child for child in childrens]\n",
    "    #print(\"Checking for : \",tokens)\n",
    "    if len(child_tokens) == 0: return []\n",
    "    else:\n",
    "        for child in child_tokens:\n",
    "                if child.pos_ in [\"NOUN\",\"PROPN\"]: \n",
    "                    important_childrens.append(child.text)\n",
    "                important_childrens += extract_important_childrens(child.children)\n",
    "        \n",
    "        return important_childrens\n",
    "    \n",
    "               \n",
    "def extract_important_words(sent):\n",
    "    tokens = nlp(sent);\n",
    "    \n",
    "    pos_constraints = [\"NOUN\",\"PROPN\",\"ADJ\"]\n",
    "    nsubjs = []\n",
    "    nsubjs_childrens = []\n",
    "    objs = []\n",
    "    objs_childrens = []\n",
    "    roots = []\n",
    "    roots_childrens = []\n",
    "    amods = []\n",
    "\n",
    "    for token in tokens:\n",
    "        \n",
    "        if token.dep_ in [\"nsubj\",\"nsubjpass\"] and token.pos_ in pos_constraints:#and token.pos_ in [\"NOUN\",\"PROPN\"]: \n",
    "\n",
    "            nsubjs.append(token.text)\n",
    "            nsubjs_childrens += extract_important_childrens(token.children)\n",
    "\n",
    "\n",
    "        elif token.dep_ in [\"obj\",\"dobj\",\"iobj\",\"pobj\"] and token.pos_ in pos_constraints: \n",
    "            objs.append(token.text)\n",
    "            objs_childrens += extract_important_childrens(token.children)\n",
    "            \n",
    "        elif token.dep_ in [\"ROOT\"] and token.pos_ in pos_constraints:\n",
    "            roots.append(token.text)\n",
    "            roots_childrens += extract_important_childrens(token.children)\n",
    "            \n",
    "            \n",
    "    return list(set(nsubjs+nsubjs_childrens+objs+objs_childrens+roots+roots_childrens))\n",
    "\n",
    "# if important words are empty, we fall back on vectorizing all the words\n",
    "def important_words_to_vec(sent):\n",
    "    important_words = extract_important_words(sent)\n",
    "        \n",
    "    vs = np.zeros(100)\n",
    "    num_words = 0    \n",
    "    words_to_iterate = important_words if len(important_words) > 0 else [token.text for token in nlp(sent)]\n",
    "    for word in words_to_iterate: \n",
    "        if word in model:\n",
    "            num_words+=1\n",
    "            vs = np.add(vs,model[word])\n",
    "\n",
    "    if num_words > 0: vs = np.divide(vs, num_words)\n",
    "    return vs\n",
    "\n",
    "def print_tokens_info(sent):\n",
    "    tokens = nlp(sent)\n",
    "    for token in tokens: \n",
    "        print(token.text,token.pos_,token.dep_,token.head)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lowincome individuals may qualify for additional benefits such as the s are collected to confirm the amount of canada_pension_plan benefits date_of_birth marital_status and date of death'"
      ]
     },
     "execution_count": 1688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0,len(quad_stream)) \n",
    "statement = quad_sents[idx];statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'canada_pension_plan', 'benefits', 'individuals', 'amount', 'death']"
      ]
     },
     "execution_count": 1689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_important_words(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a DET det tool\n",
      "selfserve NOUN compound tool\n",
      "tool NOUN ROOT tool\n",
      "that ADJ nsubj gives\n",
      "gives VERB relcl tool\n",
      "you PRON dative gives\n",
      "a DET det list\n",
      "customized ADJ amod list\n",
      "list NOUN dobj gives\n",
      "of ADP prep list\n",
      "federal_and_provincial ADJ amod programs_and_services\n",
      "or CCONJ cc federal_and_provincial\n",
      "territorial ADJ conj federal_and_provincial\n",
      "programs_and_services NOUN pobj of\n",
      "for ADP prep be\n",
      "which ADJ pobj for\n",
      "you PRON nsubj be\n",
      "may VERB aux be\n",
      "be VERB relcl programs_and_services\n",
      "eligible ADJ acomp be\n"
     ]
    }
   ],
   "source": [
    "print_tokens_info(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42334065, -0.39303927, -1.68439957,  0.75216232,  0.801535  ,\n",
       "       -0.08767149, -0.37712129,  0.34931072, -0.30723678,  0.00643667,\n",
       "       -0.83301665,  0.79181074,  0.54243128, -1.0890391 , -0.77291706,\n",
       "        0.15547569,  0.47596643,  0.8113825 , -0.18547491,  0.91664824,\n",
       "        0.89538675, -0.17433961, -0.30484614, -0.60799384, -0.42346805,\n",
       "       -0.39190262, -0.5280691 , -0.17095338,  0.17808194,  0.30573152,\n",
       "       -0.01844273,  0.12201561,  1.16041741, -0.26933178, -0.49526895,\n",
       "       -1.91150582,  0.92996629, -0.09747603, -0.1720103 ,  0.37293532,\n",
       "        0.72227428, -0.78694876, -0.08724567,  0.33885501, -0.41784123,\n",
       "       -0.16138673, -0.16985127, -1.25440089,  1.01665209,  1.91380348,\n",
       "        1.36372462,  0.1645832 ,  0.11790471,  1.12049412, -0.59503786,\n",
       "        0.02672683,  0.30838578,  0.45770224, -0.55125607, -0.56056036,\n",
       "        0.18201169, -1.49064491, -0.55765053, -0.60381844,  0.0585424 ,\n",
       "        0.229584  ,  1.3475775 , -0.56016631, -0.03973048,  0.2336047 ,\n",
       "       -0.2369309 , -0.8172374 ,  0.71307699, -0.27695704, -1.15167788,\n",
       "        0.14044942, -0.64821438,  0.75083051,  0.78260642,  0.56937   ,\n",
       "        1.13496709, -0.17364331, -0.48911725,  1.76314741,  0.80339789,\n",
       "       -1.38172571,  1.33505592, -1.12603369,  1.06221092, -0.33183029,\n",
       "        0.54235696, -0.82594807,  0.80147561, -0.11434587,  0.72125233,\n",
       "        0.2406094 ,  1.19175163, -1.45689323, -0.71800073, -0.21802831])"
      ]
     },
     "execution_count": 1284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words_to_vec(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift+LexRank Authors ALEXxWASSIM to be published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ESDC delivers a range of programs and services that affect Canadians throughout their lives. The Department provides seniors with basic income security, supports unemployed workers, helps students finance their post-secondary education and assists parents who are raising young children. The Labour Program contributes to social and economic well-being by fostering safe, healthy, fair and inclusive work environments and cooperative workplace relations in the federal jurisdiction. Service Canada helps citizens access ESDC's programs, as well as other Government of Canada programs and services.\n",
    "In particular, the Department is responsible for delivering over $120 billion in benefits directly to individuals and organizations through such Government of Canada programs and services as Employment Insurance, Old Age Security, the Canada Pension Plan and the Canada Student Loans Program. The Department also provides $1.8 billion in funding to other orders of government, educators and organizations in the voluntary and private sectors.\n",
    "To fulfill its mission, the Department is responsible for:\n",
    "developing policies that ensure all can use their talents, skills and resources to participate in learning, work and their community;\n",
    "delivering programs that help Canadians move through life's transitions, from school to work, from one job to another, from unemployment to employment, from the workforce to retirement;\n",
    "providing income support to seniors, families with children and Employment Insurance beneficiaries;\n",
    "fostering inclusive growth by providing opportunity and assistance to Canadians with distinct needs, such as Indigenous people, people with disabilities, homeless people and recent immigrants;\n",
    "overseeing labour relations, occupational health and safety, labour standards, employment equity and workers' compensation in the federal jurisdiction; and\n",
    "delivering programs and services on behalf of other departments and agencies, such as passport services delivered on behalf of Immigration, Refugees and Citizenship Canada and services to veterans delivered on behalf of Veterans Affairs Canada.\n",
    "ESDC assisted millions of Canadians in 2015-2016\n",
    "There were 78.5 million visits to the Service Canada website.\n",
    "Over 2 million calls were answered by 1 800 O-Canada agents.\n",
    "There were 8.7 million in-person visits to Service Canada Centres.\n",
    "4.6 million passports were issued.\n",
    "2.95 million applications were processed for Employment Insurance (initial and renewal); 690,000 for the Canada Pension Plan; 775,000 for Old Age Security.\n",
    "24.7 million payments were issued for Employment Insurance (initial and renewal); 64.4 million for the Canada Pension Plan; 68.5 million for Old Age Security.\n",
    "18.6 million Employment Insurance enquiries and 3.3 million enquiries related to the Canada Pension Plan and Old Age Security were resolved in the Interactive Voice Response system.\n",
    "Service Canada Call Centre agents answered 3.4 million Employment Insurance calls, 2.5 million Canada Pension Plan and Old Age Security calls and 500,000 calls related to employer services.\n",
    "640,000 full-time post-secondary students received federal student financial assistance, which includes students who received a Canada Student Loan, a Canada Student Grant and/or those who benefited from an in-study interest subsidy.\n",
    "$3.27 billion was withdrawn from Registered Education Savings Plans for 395,027 students to help fund their post-secondary education.\n",
    "94 percent of labour disputes in federally regulated workplaces were settled without a work stoppage as part of the collective bargaining process.\n",
    "98.9 percent of initial Wage Earner Protection Program payments and non-payment notifications were issued within the 42-day service standard.\n",
    "Included in these core roles are responsibilities for the design and delivery of some well-known Government of Canada programs and services:\n",
    "Old Age Security;\n",
    "the Canada Pension Plan;\n",
    "Employment Insurance;\n",
    "the Canada Student Loans and Grants and Canada Apprentice Loans Program;\n",
    "the Canada Education Savings Program;\n",
    "the Wage Earner Protection Program; and\n",
    "passport services.\n",
    "Service standards\n",
    "For 2017-2018, the following are our key service commitments:\n",
    "80% of EI benefit payments or non-payment notifications issued within 28 days of filing\n",
    "90% of OAS basic benefits paid within the first month of entitlement\n",
    "90% of CPP retirement benefits paid within the first month of entitlement\n",
    "80% of CPP Disability initial application decisions made within 120 calendar days of receipt of a completed application\n",
    "80% of EI, CPP, OAS and Employer Contact Centre calls answered by an agent within 10 minutes\n",
    "95% payment accuracy for EI, CPP and OAS\n",
    "90% of grants and contributions proposals are acknowledged within 21 calendar days of receiving an application package\n",
    "90% of contribution payments are processed within 28 calendar days of receiving a completed claim package\n",
    "90% of first installment grant payments processed no later than 15 calendar days after the approved project start date\n",
    "90% of passports issued on time\n",
    "Direct benefits to Canadians are part of Canada's social safety net and represent 95 percent of the Department's expenditures.\n",
    "Through the Labour Program, the Department contributes to the well-being of working Canadians by providing labour relations mediation services, enforcing minimum working conditions, promoting decent work and fostering respect for international labour standards.\n",
    "Through Service Canada, the Department helps Canadians access departmental programs as well as other Government of Canada programs and services at 589 in-person points of service across the country (555 Service Canada points of service, 2 consolidated offices with a Passport office and 32 stand-alone Passport offices). In addition to in-person services, the organization also serves the needs of Canadians online at Canada.ca, through My Service Canada Account and by telephone through 1 800 O-Canada and its network of call centres.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 1580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sent_tokenize(text)\n",
    "documents = list(set(sents))\n",
    "documents = [re.sub('[^a-zA-Z0-9\\s]+', '', sent).lower() for sent in sents];len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs = [important_words_to_vec(sent) for sent in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch Approach\n",
    "\n",
    "def gaussian(d,bw):\n",
    "    return np.exp(-0.5*((d/bw))**2 / (bw*math.sqrt(2*math.pi)))\n",
    "\n",
    "# (bw == kernel width) should cover 1/3 of your data\n",
    "def meanshift(data,bw=1,iterations = 5):\n",
    "    X = np.copy(data)\n",
    "    for it in range(iterations):\n",
    "        for i,x in enumerate(X):\n",
    "            dist = np.sqrt((x-X)**2).sum(1)\n",
    "            weight = gaussian(dist,bw)\n",
    "            X[i] = (np.expand_dims(weight,1)*X).sum(0) / weight.sum()\n",
    "    return X\n",
    "\n",
    "def get_unique_vecs(x):\n",
    "    seen = set()\n",
    "    centroids = []\n",
    "    for item in x:\n",
    "        t = tuple(item)\n",
    "        if t not in seen:\n",
    "            centroids.append(item)\n",
    "            seen.add(t)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "bandwidth = estimate_bandwidth(document_vecs, quantile=0.33)\n",
    "\n",
    "mean_shifted = meanshift(document_vecs,bw=bandwidth,iterations=15);\n",
    "len(get_unique_vecs(mean_shifted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift,estimate_bandwidth\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_documents_by_similarity(docs,bandwidth_quantile=0.3):\n",
    "    # Returns a tuple \n",
    "    # 1.the labels to the list of document ids\n",
    "    # 2. the vectors grouped by labels\n",
    "    \n",
    "    bandwidth = estimate_bandwidth(docs, quantile=bandwidth_quantile)\n",
    "    if bandwidth == 0: print(\"Not enough documents to separate them.\")\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=False)\n",
    "    ms.fit(docs)\n",
    "    num_centroids = len(ms.cluster_centers_)\n",
    "\n",
    "    labels_to_ids = [set() for _ in range(num_centroids)]\n",
    "    for doc_id,label in enumerate(ms.labels_): labels_to_ids[label].add(doc_id)\n",
    "        \n",
    "    doc_vec_labeled = np.array([(vec,l) for vec,l in zip(docs,ms.labels_)])\n",
    "    docs_grouped = [doc_vec_labeled[doc_vec_labeled[:,1] == i,0] for i in range(num_centroids)]\n",
    "    return labels_to_ids,docs_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_ids,docs_grouped = group_documents_by_similarity(document_vecs,bandwidth_quantile=0.3);len(label_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_parts = [np.array(documents)[list(ids)] for _,ids in enumerate(label_to_ids)]\n",
    "doc_parts = [\". \".join(doc_parts[label])+\".\" for label,_ in enumerate(label_to_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in particular the department is responsible for delivering over 120 billion in benefits directly to individuals and organizations through such government of canada programs and services as employment insurance old age security the canada pension plan and the canada student loans program.\n",
      "\n",
      "the department provides seniors with basic income security supports unemployed workers helps students finance their postsecondary education and assists parents who are raising young children.\n",
      "\n",
      "over 2 million calls were answered by 1 800 ocanada agents.\n",
      "\n",
      "327 billion was withdrawn from registered education savings plans for 395027 students to help fund their postsecondary education.\n",
      "\n",
      "640000 fulltime postsecondary students received federal student financial assistance which includes students who received a canada student loan a canada student grant andor those who benefited from an instudy interest subsidy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bullet_points = []\n",
    "for text in doc_parts:\n",
    "    num_sentences = len(sent_tokenize(text))\n",
    "    if(num_sentences <= 1):\n",
    "        bullet_points.append(text)\n",
    "    else:\n",
    "        summarize_1_sent_ratio = int(100*float(1/num_sentences))/100\n",
    "        lead_sents = summarize(text,ratio=summarize_1_sent_ratio,split=True)\n",
    "        if len(lead_sents) > 0:\n",
    "            bullet_points.append(summarize(text,ratio=summarize_1_sent_ratio,split=True)[0])\n",
    "            \n",
    "    \n",
    "for point in bullet_points:\n",
    "    print(point+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
